# Working with exceptions on your ZTP/RHACM infrastructure

[Red Hat ACM](https://www.redhat.com/en/technologies/management/advanced-cluster-management) allows you to deploy, upgrade, configure different Spoke clusters, from a Hub cluster. It is an Openshift cluster that manages other clusters.

This document will show how to use RHACM and the Governance model, to manage the configuration and policies of your clusters, but using exceptions on some specific clusters. Previously, we have seen a similar scenario about creating subgroups: [Working with subgroups and configurations on your ZTP/Red Hat Advanced Cluster Management for Kubernetes infrastructure](https://www.redhat.com/en/blog/working-subgroups-and-configurations-your-ztpred-hat-advanced-cluster-management-kubernetes-infrastructure).

Please, consider to read the previous document to have more context and better understanding on how RHACM and ZTP work. Basic knowledge on these technologies is required to follow this document. 

The [PolicyGenTemplates](https://docs.openshift.com/container-platform/4.12/scalability_and_performance/ztp_far_edge/ztp-configuring-managed-clusters-policies.html) easiness the definition of the Policies that will manage the configuration of your infrastructure. A PolicyGenTemplate defines a set of configurations/policies, and these are linked to different clusters:

```yaml
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "group-du-sno"
  namespace: "ztp-group"
spec:
  bindingRules:
    group-du-sno: ""
    
<CONFIGURATIONS/POLICIES>

```

We use PGTs to easiness the creation of Policies, and manage all the different configurations of our infrastructure. In this example, all the clusters with the `group-du-sno:""` label, will be affected by the policies generated by this PGT.

But, what happens if we need to make exceptions? What if a cluster, that logically belongs to that group, needs to be configured exceptionally? 

This is the objective of this article, to be able to set exceptions, but making the cluster still belonging to its logical groups. Also, the exceptional behaviour of that cluster would be reverted. 

Some differences with our previous article:
 * Instead of creating subgroups or clusters, we will set some exceptions on configurations.
 * Instead of using RHACM Policies, we will use ZTP PGTs.
 * Instead of a Canary rollout, when the new configurations is tested, the exceptional cluster is reverted to the default configuration of the group. In our previous document we did the other way around. We moved all the other clusters to the new tested configuration.

There is no intention on demonstrate which way of working is better. Depending on the scenario, to work with subgroups or exceptions would be better. The same about PGTs, Policies, or the Canary rollout, here we are using different options, to instruct about the different available tools.

## The scenario

For this tutorial we will focus on an scenario with three Single Node Openshift (SNOs). All of them are intended to be used in a telco environment to deploy a Midband Distribution Unit. But this is just an example, and the way of proceeding can apply to whatever other scenario with ZTP having to implement exceptions on some configurations. 

SNO5 and SNO6 are already deployed and working. Both are based on OCP4.12 and have been configured for a Midband DU. In their configuration, we can see the groups they belong to and the OCP version.

![](assets/working_with_exceptions_on_ZTP_clusters-installed.png)


From the RHACM GUI, we can see they belong to the groups: `common` and `group-du-mb`. In addition, we can see the label `ztp-done`, that confirms the clusters have been correctly configured by ZTP. And therefore, workloads can be deployed. 

Going further about Polices, we can also see, the different configurations (Policies) that have been applied. 

<img src="assets/2023-06-23-10-38-41-image.png" title="" alt="" width="392">

With this scenario as base, we want to introduce a new SNO for a MB DU. But now, we want to test our configurations with OCP4.13. The idea, is to start experimenting if the workloads to be deployed can run in this environment. With the intention of detecting possible issues and miss configurations. Therefore, we need to reach to the `ztp-done` considering that everything would not be 100% available yet, and, some exceptions would be considered.

The implemented mechanisms has to:

* To not affect the current deployments and configurations. Here, for demoing reasons, we are using only 3 SNOs and some few configurations. Imagine scenarios with hundreds of clusters, and dozens of configurations. 

* To allow exceptions on some configurations. At the same time, we cannot make the infrastructure definition and configuration more complex.

* The cluster with exceptions belongs to the groups that should belong. In this case, the cluster is part of the group `group-du-mb`. No matter if it contains exceptions. From a logical and functional point of view, the cluster is a DU MB.

* The exceptions can be, easily, reverted. 

## Adding a new cluster for experimentation

As we have explained above, our new cluster SNO7 will be deployed using OCP4.13.

![](assets/working_with_exceptions_on_ZTP_sno7.png)

The cluster installation with OCP4.13 has been done correctly, and the cluster configuration is managed by the labels: `common` and `group-du-mb`. But we can also see, in the labels, how `ztp-running` point us that it is not fully configured. It is not ready for day-2 and workloads deployments.

If we take a look to the Policies on RHACM, we can see the Policies that are trying to be applied.

<img src="assets/2023-06-23-10-57-54-image.png" title="" alt="" width="600">

1) These three first Policies install all the configurations for the clusters labelled as common. It is expected this will work. Common policies are pretty generic to all environments.

2) These are the failing ones. Without going very much on details, `sno7-du-mb-op-conf-fec-operator-lg7qs` is the one trying to remediate the Policy `du-mb-op-fec-operator`. And for some reason is failing.

3) These Policies are not even trying to be applied. These are awaiting the previous one to succeed, before continuing. This is default behavior remediating Policies.

The reason for the failure, it is an operator that dont exist (yet) on the OCP 4.13 catalog. For testing our cluster on 4.13, we would proceed without installing this operator. We will create an exception, that will allow us to fully configure the new cluster, but this operator. When the operator is available, we would remove the exception.

## Creating the configuration exceptions

The `du-mb-op-fec-fec-operator`Policy comes from a PolicyGenTemplate in our GitOps repository:

```yaml
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-mb-op-fec"
  namespace: "ztp-group"
spec:
  bindingRules:
    group-du-mb: ""
  mcp: "master"
  sourceFiles:
    # Install fec operator as part of the DU
    - fileName: AcceleratorsNS.yaml
      policyName: "fec-operator"
    - fileName: AcceleratorsOperGroup.yaml
      policyName: "fec-operator"
    - fileName: AcceleratorsOperatorStatus.yaml
      policyName: "fec-operator"
    - fileName: AcceleratorsSubscription.yaml
      policyName: "fec-operator"


```

And this Operator dont exists, yet, on our new OCP 4.13. We can observe this on the details of the Subscription (on the SNO7 cluster) that is trying to be created:

```bash
[SNO7]$ oc -n vran-acceleration-operators get subscriptions.operators.coreos.com sriov-fec-subscription  -o jsonpath={.status.conditions[1]} | jq
{
  "message": "constraints not satisfiable: no operators found in package sriov-fec in the catalog referenced by subscription sriov-fec-subscription, subscription sriov-fec-subscription exists",
  "reason": "ConstraintsNotSatisfiable",
  "status": "True",
  "type": "ResolutionFailed"
}

```

We can modify the PolicyGenTemplate conditions about which clusters will be affected. Currently, all the clusters with `group-du-mb` are affected, but we will add an exception in the PGT:

```yaml
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-mb-op-fec"
  namespace: "ztp-group"
spec:
  bindingRules:
    group-du-mb: ""
  bindingExcludedRules:
    experiment: ""
```

Now, the PGT will affect to the clusters in the group `group-du-mb`, except for the ones that also belong to the group `experiment`. Our cluster still belongs to the `group-du-mb` but it is not going to be affected. 

Of course, we also need to label the cluster with this new label `experiment`. In order to do that, we edit the Siteconfig of the SNO7 in our GitOps repository.

![](assets/2023-06-23-11-26-50-image.png)

## Checking the new status

With the new exception added, the cluster is not affected by the Policy `du-mb-op-fec-operator`. And the configuration will continue applying the configurations related to this cluster. 

![](assets/2023-06-23-12-23-10-image.png)

We have reached the final validation and the cluster is `ztp-done`. We can start deploying workloads.

## Revert back to a normal configuration

Just removing the label `experiment` from the Siteconfig will make the cluster, to be affected by all the Policies of the group `group-du-mb`. So, to revert our exceptionally is very easy to achieve. 

## Conclusions

Using cluster's labels, `bindingExcludedRules` and `bindingRules`, we can work at very detailed level about how we want to configure our clusters. In this tutorial, we have seen how to implement exceptions for clusters groups, not having to create multiple copies of a Policy for different groups or subgroups. The exceptions allow us to skip some configurations, with an easy way of going back to a normal configuration.

For a second tutorial, we will go further about using these exceptions. Not only to skip configurations, but also, to apply a different configuration with the exception. For example, in the scenario covering in this tutorial, an exception could install a new `Catalogsource`which contains that missing operator.




