 [draft][ongoing]

# Scaling down clusters created with Zero Touch Provisioning

Here, Zero Touch Provisioning or ZTP, is a set of tools that helps you to deploy Red Hat Openshift (OCP) clusters. Mainly, it is composed by ACM, Openshift Assisted Installer and ArgoCD with a set of specific Kustomize plugins. These plugins, allows you to define your clusters on a single manifest called Siteconfig. More info [here](https://docs.openshift.com/container-platform/4.11/scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-clusters-at-scale.html)

The whole ZTP stack is composed by many different, independent components. ZTP Plugins, ArgoCD, ACM, Assisted Installer, Baremetal Operator. 

In this combination we still miss a feature that allows to scale down clusters. It would seems a little strange that scaling up is allowed, but not scale down. There is a  [request to implement that feature](https://issues.redhat.com/browse/RFE-3431), but not yet implemented at the time of written this article.

The Assisted Installer deploys an agent service in each host during installation. But after that, the agent finishes and the service is out. We would say that Agent Installer is cool for installing, but it is not aware of anything happening in the cluster after the installation. It is true, that ACM, also deploys a living agent in each host, but ACM is not interacting with ACM in that way. **Actually, I am not sure if this ACM agent provides that kind of functionality**



## Understanding the installation

When you deploy a cluster using ZTP, you have to define your Siteconfig Manifest that will initiate the deployment. Gitops part will take the Siteconfig information to generate the different manifests needed by the other components (ACM, Assisted Installer, Baremetal Operator).

[Here](https://docs.openshift.com/container-platform/4.11/scalability_and_performance/ztp_far_edge/ztp-manual-install.html#ztp-installation-crs_ztp-manual-install) the list of generated manifests/resources.

The most relevant parts are:

* BareMetalHost/BMH that contains information of the BMCs of the different servers to be installed. With this info, the BaremetalOperator can boot servers with an installation ISO.

* The BMH are incorporated into an Infraenv resource, which contains common information for all the hosts, about the infrastructure.

* AgentClusterInstall, contains information of the clusters and the different hosts.

All these three objects (and some others more) are generated by the Siteconfig. And all of the are under the ZTP Gitops control. You would modify Siteconfig adding and deleting hosts. And the corresponding objects will be deleted from the cluster.

But there is one extra, very important, generated object: Agent. When each host boots with the installation ISO, and agent service is installed. These agent service will collect information from each host, it will register the host into the AgentClusterInstall, and it will start the OCP installation. But also, in the Management Cluster. In this moment, the Agent Object appears also in the management/hub cluster.

So we have: BMH, Infraenv, AgentClusterInstall and Agent. Together with other objects, not so relevant in this tutorial.

## Scaling down clusters

BMH, Infraenv and AgentClusterInstall are created by the Siteconfig and the GitOps flow. So, you would modify the Siteconfig to interact with them. 

In principle, if you delete a host in the SiteConfig, this will delete its corresponding BMH object. And it would be expected to delete the host. But it is not.

* The Assisted Installer contains an Openshift/Kubernetes controller. When a BMH is created, the controller and the Assisted Installer created the installation images for each BMH.

* The controller is not looking for BMHs deletion. IBut, should the Assisted Installer covers also uninstalling functionality. Maybe not.

* BMH CRs are exposed by the BareMetalOperator, which is aware of this deletion. But it does not make any uninstalling process.

Following some tests of what is happening when try to delete an object.

### Trying to delete a host

In this example we can see a cluster with 3 masters and two workers, and the corresponding Agents and BMHs:

```bash
$> oc -n el8k-ztp-1 get agent,bmh
NAME                                                                    CLUSTER      APPROVED   ROLE     STAGE
agent.agent-install.openshift.io/2f03b103-596c-4736-aeed-289e590a8bb0   el8k-ztp-1   true       worker   Done
agent.agent-install.openshift.io/42a4c6df-1598-1e66-efb1-16a1712c8362   el8k-ztp-1   true       master   Done
agent.agent-install.openshift.io/64fee173-6d1b-43bc-ae9d-80b72ee57e6a   el8k-ztp-1   true       worker   Done
agent.agent-install.openshift.io/b023e155-14fa-3c24-c6d7-0b07a0cf6dde   el8k-ztp-1   true       master   Done
agent.agent-install.openshift.io/feeda90a-75c8-c1bb-3ba2-88fb4be2d2b5   el8k-ztp-1   true       master   Done

NAME                                                       STATE         CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/master-0.el8k-ztp-1.hpecloud.org   provisioned              true             28h
baremetalhost.metal3.io/master-1.el8k-ztp-1.hpecloud.org   provisioned              true             28h
baremetalhost.metal3.io/master-2.el8k-ztp-1.hpecloud.org   provisioned              true             28h
baremetalhost.metal3.io/worker-0.el8k-ztp-1.hpecloud.org   provisioned              true             43m
baremetalhost.metal3.io/worker-1.el8k-ztp-1.hpecloud.org   provisioned              true             43m

```

You can see the same from the GUI:

![](assets/2022-12-15-18-07-51-image.png)

Also, you can observe you cannot delete the host, because it is already installed. This functionality is not possible.  But you can remove it from the cluster. What does this exactly means?

![](assets/2022-12-15-18-11-12-image.png)

The host is removed from the cluster but its BMH still belongs to the Infraenv. You can see the Infraenv as a pool of hosts. After removing it from the cluster, the host still belongs to the Infraenv and it is released. It could be used in a new installation. 

Clarification: This way of using Infraenvs as a pools of hosts is not the approach followed by ZTP. With ZTP you define a set of hosts, an Infraenv is created with all the hosts, and all the hosts are added to the cluster you are creating. It is not designed to exploit this functionality of releasing and moving hosts between clusters.

Anyway, we cannot delete the host, but we will remove it.

![](assets/2022-12-15-18-16-14-image.png)

The host has been released:

```bash
> oc -n el8k-ztp-1 get agent,bmh
NAME                                                                    CLUSTER      APPROVED   ROLE     STAGE
agent.agent-install.openshift.io/2f03b103-596c-4736-aeed-289e590a8bb0   el8k-ztp-1   true       worker   Done
agent.agent-install.openshift.io/42a4c6df-1598-1e66-efb1-16a1712c8362   el8k-ztp-1   true       master   Done
agent.agent-install.openshift.io/64fee173-6d1b-43bc-ae9d-80b72ee57e6a                true       worker   Done
agent.agent-install.openshift.io/b023e155-14fa-3c24-c6d7-0b07a0cf6dde   el8k-ztp-1   true       master   Done
agent.agent-install.openshift.io/feeda90a-75c8-c1bb-3ba2-88fb4be2d2b5   el8k-ztp-1   true       master   Done

NAME                                                       STATE         CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/master-0.el8k-ztp-1.hpecloud.org   provisioned              true             29h
baremetalhost.metal3.io/master-1.el8k-ztp-1.hpecloud.org   provisioned              true             29h
baremetalhost.metal3.io/master-2.el8k-ztp-1.hpecloud.org   provisioned              true             29h
baremetalhost.metal3.io/worker-0.el8k-ztp-1.hpecloud.org   provisioned              true             58m
baremetalhost.metal3.io/worker-1.el8k-ztp-1.hpecloud.org   provisioned              true             12m

```

Also, in the cluster we should not see that worker-0:

![](assets/2022-12-15-18-20-35-image.png)

But sadly, the worker has not been really deleted from cluster:

```bash
> oc get nodes
NAME                               STATUS   ROLES           AGE     VERSION
master-0.el8k-ztp-1.hpecloud.org   Ready    master,worker   28h     v1.23.12+8a6bfe4
master-1.el8k-ztp-1.hpecloud.org   Ready    master,worker   28h     v1.23.12+8a6bfe4
master-2.el8k-ztp-1.hpecloud.org   Ready    master,worker   28h     v1.23.12+8a6bfe4
worker-0.el8k-ztp-1.hpecloud.org   Ready    worker          49m     v1.23.12+8a6bfe4
worker-1.el8k-ztp-1.hpecloud.org   Ready    worker          3m40s   v1.23.12+8a6bfe4
```

Who should be deleting the worker? Something like 'oc delete node worker-0'. 



What if we delete the BMH and Agent? The GUI does not allows us to do that. Instead of that, we can release it. The approach is to releas a host that can be re-used later. With ZTP and GitOps we would delete and recreate again when needed. Different approaches. Anyway, lets just delete the two objects. This time we use the worker-1

```bash
> oc -n el8k-ztp-1 delete agent 2f03b103-596c-4736-aeed-289e590a8bb0 
agent.agent-install.openshift.io "2f03b103-596c-4736-aeed-289e590a8bb0" deleted
> oc -n el8k-ztp-1 delete bmh worker-1.el8k-ztp-1.hpecloud.org
baremetalhost.metal3.io "worker-1.el8k-ztp-1.hpecloud.org" deleted
```

If we go back to the GUI and the Infraenv:

![](assets/2022-12-15-18-29-15-image.png)

We really see the worker-1 disappeared. It was not released. If we take a look to the cluster:

![](assets/2022-12-15-18-30-46-image.png)

It has been also removed from there. But, of course, no one was in charge of really telling the cluster to do the oc delete:

```bash
> oc get nodes
NAME                               STATUS   ROLES           AGE   VERSION
master-0.el8k-ztp-1.hpecloud.org   Ready    master,worker   28h   v1.23.12+8a6bfe4
master-1.el8k-ztp-1.hpecloud.org   Ready    master,worker   29h   v1.23.12+8a6bfe4
master-2.el8k-ztp-1.hpecloud.org   Ready    master,worker   29h   v1.23.12+8a6bfe4
worker-0.el8k-ztp-1.hpecloud.org   Ready    worker          59m   v1.23.12+8a6bfe4
worker-1.el8k-ztp-1.hpecloud.org   Ready    worker          13m   v1.23.12+8a6bfe4

```

Neither the Assisted Installer (controlling the Agent), nor the BareMetalOperator (controlling the BMH) really deleted, or removed, the host from the cluster.

**[ToDo] maybe some logs from the BMO to see what is happening*

### Automated Cleaning Mode on BMH

*[Todo] to look about this feature, disabled by default on ZTP*



### Deleting hosts with ZTP

As explained above, ZTP and Gitops are only controlling AgentClusterInstall, BMH and Infraenv. If you delete a host from your Siteconfig, the only resource that ZTP will try to delete is the corresponding BMH.

**[ToDo] what happens if you only delete the BMH**







## Scaling down clusters with ZTP

As explained above, there are still some [missing features](https://issues.redhat.com/browse/RFE-3431) that allows us a better removing, re-provisioning or scaling down of clusters. But some of our partners needs this functionality. Meanwhile, we can use ZTP, Policies and some manual work as a workaround.

**[ToDo] Write the WA**




