<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>chatpgt on Jose Gato Blog</title>
    <link>https://jgato.github.io/jgato/tags/chatpgt/</link>
    <description>Recent content in chatpgt on Jose Gato Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 21 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jgato.github.io/jgato/tags/chatpgt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>InstructLab for creating your own &#39;ChatPGT&#39; like model</title>
      <link>https://jgato.github.io/jgato/posts/instructlab/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://jgato.github.io/jgato/posts/instructlab/</guid>
      <description>My first steps with InstructLab and Granite model Recently Red Hat and IBM released (as Opensource) the project (and community) instructLab. A project that aims to work with LLM models, such as Granite and Merlinite. Both models are also released as Opensource by IBM. Granite trained over the LLM model Meta’s Llama2-7B, and Merlinite, fine tuned from Mistral.
Instructlab, helps you to download, serve and interact with LLM models. Similar to other projects like ollama.</description>
      <content>&lt;h1 id=&#34;my-first-steps-with-instructlab-and-granite-model&#34;&gt;My first steps with InstructLab and Granite model&lt;/h1&gt;
&lt;p&gt;Recently Red Hat and IBM released (as Opensource) the project (and community) &lt;a href=&#34;https://github.com/instructlab/instructlab&#34;&gt;instructLab&lt;/a&gt;. A project that aims to work with LLM models, such as &lt;a href=&#34;https://huggingface.co/ibm-granite/granite-7b-base&#34;&gt;Granite&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/ibm/merlinite-7b&#34;&gt;Merlinite&lt;/a&gt;. Both models are also released as Opensource by IBM. Granite trained over the LLM model  Meta’s Llama2-7B, and Merlinite, fine tuned from Mistral.&lt;/p&gt;
&lt;p&gt;Instructlab, helps you to download, serve and interact with LLM models. Similar to other projects like &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama&lt;/a&gt;. With the main difference, about hleping on retraining (fine tune). It allows an easy way to inject more knowledge into an existing models, without needing AI skills. You can teach the model with knowledge and features.&lt;/p&gt;
&lt;p&gt;In this blog I just want to give a quick overview on my first steps. In the futere, I would like to train the model, with the context of my own data-sets (documents, notes, etc), everything locally running (100% Opensource), as &amp;ldquo;my second brain&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;installing-instructlab-and-downloading-granite-model&#34;&gt;Installing instructLab and downloading Granite model&lt;/h2&gt;
&lt;p&gt;I will not go on how to install instructLab, because it is very well explained &lt;a href=&#34;https://github.com/instructlab/instructlab?tab=readme-ov-file#-installing-ilab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During this tutorial I am using Merlinite model, but you could also try Granite one. This is the way of downloading it:&lt;/p&gt;
&lt;p&gt;To download Granite:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ilab download --repository instructlab/granite-7b-lab-GGUF --release main --filename granite-7b-lab-Q4_K_M.gguf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And create a &lt;code&gt;config.yaml&lt;/code&gt; with the new model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;chat&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;context&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;greedy_mode&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;logs_dir&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data/chatlogs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;model&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;granite-7b-lab-Q4_K_M&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;model_type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;llama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;session&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;vi_mode&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;visible_overflow&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;general&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;log_level&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;INFO&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;generate&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;chunk_word_count&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;model&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;granite-7b-lab-Q4_K_M&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;model_type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;llama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;num_cpus&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;num_instructions&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;output_dir&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;generated&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;prompt_file&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prompt.txt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;seed_file&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;seed_tasks.json&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;taxonomy_base&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;origin/main&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;taxonomy_path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;taxonomy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;serve&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;gpu_layers&lt;/span&gt;: -&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;host_port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;127.0.0.1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;max_ctx_size&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4096&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;model_path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;models/granite-7b-lab-Q4_K_M.gguf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now in one terminal, serve (&lt;code&gt;ilab serve&lt;/code&gt;) the model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab serve --model-path models/merlinite-7b-lab-Q4_K_M.gguf                                                                                          
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-14 07:46:06,435 lab.py:340 Using model &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;models/merlinite-7b-lab-Q4_K_M.gguf&amp;#39;&lt;/span&gt; with -1 gpu-layers and &lt;span style=&#34;color:#ae81ff&#34;&gt;4096&lt;/span&gt; max context size.                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-14 07:46:08,237 server.py:206 Starting server process, press CTRL+C to shutdown server...                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-14 07:46:08,237 server.py:207 After application startup complete see http://127.0.0.1:8000/docs &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; API.  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And start chatting (&lt;code&gt;ilab chat&lt;/code&gt;) in other terminal:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab chat
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; hi, who are you?  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hello! I&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;m glad to see you. How can I assist you today? 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Nice to meet you merlinite.&lt;/p&gt;
&lt;h2 id=&#34;adding-new-knowledge-or-skills&#34;&gt;Adding new knowledge or skills&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This is just an example for playing/demoing and not trying to write the whole knowledge about SNO&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The model looks not very accurate about knowing about SNO, Single Node Openshift, that is a pity:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; what &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; you know about SNO?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;In OpenShift, SNO stands &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Source to Native Operator.&amp;#34;&lt;/span&gt; It is a component that enables the seamless deployment and management of Kubernetes native applications within OpenShift environments. The Source to 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Native Operator simplifies the process of building, deploying, and managing containerized applications by automating various tasks related to application lifecycle management.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; No, it is not. In the context of Openshift, what is an SNO? 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SNO is a well-known acronym that stands &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Simple Network Management Protocol.&amp;#34;&lt;/span&gt; It is a network protocol used by various devices to communicate and manage the status of their connections, ensuring seamless 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;communication between them. I&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;m here to help with any questions or concerns you might have about SNO or any other topic!
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;No, the model is not on the way of answering about SNO?&lt;/p&gt;
&lt;p&gt;How to add more knowledge or skills are well documented &lt;a href=&#34;https://github.com/instructlab/instructlab?tab=readme-ov-file#-creating-new-knowledge-or-skills-and-training-the-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically, you create a &lt;code&gt;qna.yaml&lt;/code&gt; over the Taxonomy (that was download during your installation, actually with &lt;code&gt;ilab init&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In my case, I will create a new knowledge, the qna format is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;version&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;task_description&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Teach the model about SNO.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;created_by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;jgato&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;domain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pop_culture&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;seed_examples&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;how do you define a single node openshift?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      About OpenShift on a single node (SNO)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      You can create a single-node cluster with standard installation methods.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      OpenShift Container Platform on a single node is a specialized
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      installation that requires the creation of a special ignition
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      configuration ISO. The primary use case is for edge computing
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      workloads, including intermittent connectivity, portable clouds, and
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      5G radio access networks (RAN) close to a base station.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      The major tradeoff with an installation on a single node is the
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      lack of high availability.&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;What is an SNO?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      A SNO is a single-node Openshift,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      or Single Node Openshift.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      A cluster composed by only one node.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      Specially designed for working at Edge,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      but it can also run on suppored clouds.&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Can you add workers to an SNO?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      Yes, a SNO allows to add an extra worker.&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;What are the requirements for an SNO?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      | **Prile**      | **vCPU**     | **Memory**  | **Storage** |
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      |----------------|--------------|-------------|-------------|
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      | Minimum        | 8 vCPU cores | 16 GB RAM   | 120 GB      |&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Which is the preferable method to deploy an SNO?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      Assisted Installer is the best way of installing an SNO&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;question&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;How many nodes have an SNO?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;answer&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      An SNO, that stands for Single Node Openshift,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      it is compossed by only one node.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      But there is one special configurarion that allows
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      to add one worker.&lt;/span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;document&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;repo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://github.com/jgato/ilab_knowledge.git&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;commit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eb5da33&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;patterns&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Meet single node OpenShift.md&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just some simple questions and answer and the reference to a document, that is just a Markdown version of a &lt;a href=&#34;https://www.redhat.com/en/blog/meet-single-node-openshift-our-smallest-openshift-footprint-edge-architectures&#34;&gt;blog talking about SNO&lt;/a&gt;. All the credits for this knowledge to: Moran Goldboim, Eran Cohen.&lt;/p&gt;
&lt;h2 id=&#34;generate-synthetic-data&#34;&gt;Generate synthetic data&lt;/h2&gt;
&lt;p&gt;Now that we have our new knowledge, we need to generate more data. ilab will use a running model to generate more synthetic data from the added knowledge.&lt;/p&gt;
&lt;p&gt;First, validate the Taxonomy with your new kwnoledge:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab diff
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;knowledge/technical_manual/openshift/sno/qna.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Taxonomy in /taxonomy/ is valid :&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And generate more data, that later, will re-train the model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab generate --model models/granite-7b-lab-Q4_K_M.gguf --num-cpus &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt; --num-instructions &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llama_cpp_python is built without hardware acceleration. ilab generate will be very slow.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Generating synthetic data using &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;models/merlinite-7b-lab-Q4_K_M.gguf&amp;#39;&lt;/span&gt; model, taxonomy:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;taxonomy&amp;#39;&lt;/span&gt; against http://127.0.0.1:8000/v1 server                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Cannot find prompt.txt. Using default prompt depending on model-family.                                                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  0%|                                                                   | 0/10 &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;00:00&amp;lt;?, ?it/s&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;Synthesizing new instructions. If you aren&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;t satisfied with the generated instructions, interru
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pt training &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Ctrl-C&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; and try adjusting your YAML files. Adding more examples may help.                                                                                                        INFO 2024-06-14 07:48:23,797 generate_data.py:506 Selected taxonomy path knowledge-&amp;gt;technical_manual-&amp;gt;openshift-&amp;gt;sno                                                                          
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                                                                                                                                              
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                                                                                                                                              
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;^&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;1;5D^&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;1;5D^&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;1;5D^&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;1;5D^&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;1;5CQ&amp;gt; What does SNO stand &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; in the OpenShift context?                                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I&amp;gt;                                                                                                                                                                                            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A&amp;gt; SNO stands &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; Single Node OpenShift, which is a new type of OpenShift deployment introduced in OpenShift version 4.9. It combines both control and worker node functionalities into a sing
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;le server, making it an ideal solution &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; edge computing workloads with limited space, power, and cooling resources.                                                                         
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                                                                                                                                                              
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; 10%|█████▉                                                     | 1/10 &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;00:55&amp;lt;08:23, 55.99s/it&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;INFO 2024-06-14 07:49:19,791 generate_data.py:506 Selected taxonomy path knowledge-&amp;gt;technical_m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;anual-&amp;gt;openshift-&amp;gt;sno    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;..
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;..
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-14 08:37:03,431 generate_data.py:608 &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt; instructions generated, &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt; discarded due to format &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;see generated/discarded_merlinite-7b-lab-Q4_K_M_2024-06-14T07_52_52.log&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt; discarded due to rouge score
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-14 08:37:03,431 generate_data.py:612 Generation took 2651.20s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From the 10 instructions generated, only a few are discarded. Good sign.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the final model, I re-generated the data with more instructions, with an iterative process of tuning the qna content and the number of instructions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;During the generation process, you will see how the model is generating questions and answers. If you dont like very how is going, maybe you will have to tune your qna.&lt;/p&gt;
&lt;p&gt;Some comments on generating data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--num-cpus&lt;/code&gt; using 24, anyway, if there is no GPU&amp;hellip; be patiente.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--num-instructions&lt;/code&gt; is about the new generative data sets created from you qna. I am using half of the default. Just an example, and without GPU it takes long time.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--model&lt;/code&gt; to specify that I am using Granite and not the default one.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;train-the-model&#34;&gt;Train the model&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice: when writing this article the training on Linux and using CPU it is pretty tricky and produces not very accurate model. I tried &lt;a href=&#34;https://github.com/instructlab/instructlab/issues/1305#issuecomment-2181257894&#34;&gt;this suggestion&lt;/a&gt; that provides me a working trained model. There are ongoing efforts and incoming features to provide better training no matter the platform. Anyway to train with a CPU is not something that you will take to train a model seriously.  But it is oka for playing and leaning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Alternatives: You can try to to train with  this &lt;a href=&#34;https://github.com/instructlab/instructlab/blob/main/notebooks/Training_a_LoRA_With_Instruct_Lab.ipynb&#34;&gt;Google Colab Notebook&lt;/a&gt;. There you can upload the generated data and train with a GPU. But, the training is more manual work, and you will not have the joy of using instructLab, that hides the complexity, and it does not require tech/AI skills.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, lets train with Linux and CPU (but remember I am using a powerful server with lots of CPU and Memory).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab train --iters &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt; --num-epochs &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;INFO 2024-06-21 03:15:08,999 config.py:58 PyTorch version 2.3.1+cpu available.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: NUM EPOCHS IS:  &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: TRAIN FILE IS:  generated/train_merlinite-7b-lab-Q4_K_M_2024-06-19T09_16_56.jsonl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: TEST FILE IS:  generated/test_merlinite-7b-lab-Q4_K_M_2024-06-19T09_16_56.jsonl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: Using device &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: LOADING DATASETS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/home/jgato/playing/instructlab/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;resume_download&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; is deprecated and will be removed in version 1.0.0. Downloads always resu
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;me when possible. If you want to force a new download, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;force_download&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  warnings.warn&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pad_token &amp;lt;unk&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: NOT USING 4-bit quantization
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: LOADING THE BASE MODEL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;00:01&amp;lt;00:00,  1.84it/s&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: Model device cpu
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: SANITY CHECKING THE BASE MODEL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;06:01&amp;lt;00:00, 60.23s/it&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: GETTING THE ATTENTION LAYERS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LINUX_TRAIN.PY: CONFIGURING LoRA
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;00:00&amp;lt;00:00, 1189.65 examples/s&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/home/jgato/playing/instructlab/venv/lib64/python3.9/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;Accelerator&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; is deprecated and will be removed in version 1.0 o
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f Accelerate: dict_keys&lt;span style=&#34;color:#f92672&#34;&gt;([&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dispatch_batches&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;split_batches&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;even_batches&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;use_seedable_sampler&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;])&lt;/span&gt;. Please pass an &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;accelerate.DataLoaderConfiguration&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; instead:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataloader_config &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoaderConfiguration&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;dispatch_batches&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, split_batches&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False, even_batches&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, use_seedable_sampler&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Wrote training_results/final/ggml-model-f16.gguf    
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;chatting-with-the-new-model&#34;&gt;Chatting with the new model&lt;/h2&gt;
&lt;p&gt;So, lets serve it (created a new config pointing the new model):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ ilab  --config ./config-custom.yaml serve
INFO 2024-06-12 05:56:51,935 lab.py:340 Using model &amp;#39;models/ggml-model-f16.gguf&amp;#39; with -1 gpu-layers and 4096 max context size.
INFO 2024-06-12 05:56:54,729 server.py:206 Starting server process, press CTRL+C to shutdown server...
INFO 2024-06-12 05:56:54,729 server.py:207 After application startup complete see http://127.0.0.1:8000/docs for API.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Is the new model able to talk about SNO?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ilab --config ./config-custom.yaml chat -m models/ggml-model-f16.gguf                                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;╭────────────────────────────────────────────────── system ──────────────────────────────────────────────────╮                                                                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;│ Welcome to InstructLab Chat w/ MODELS/GGML-MODEL-F16.GGUF &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;type /h &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; help&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;                               │                                                                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;╰────────────────────────────────────────────────────────────────────────────────────────────────────────────╯      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; hi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hello! I&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;m here to help answer any questions or concerns you might have. How can I assist you today?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; what &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; you know about SNO?      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SNO stands &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Single Node OpenShift&amp;#34;&lt;/span&gt;, which is a deployment option &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; the OpenShift container platform that allows users to deploy and manage their workloads in a single node environment. This is particularly
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;useful when implementing edge computing scenarios, where resource constraints or connectivity issues may impact the availability of centralized management clusters. In such environments, it is essential to ensure
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;that the workload remains operational with its existing configuration, even when any centralized management functionality is unavailable.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Single Node OpenShift provides users with a consistent experience across the sites where OpenShift is deployed, and it enables operators to manage their environments in the same way, regardless of the size of the
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;deployment. This consistency helps reduce the complexity and cost associated with implementing multiple versions of OpenShift tailored &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; different use cases or environments.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; which scenarios are recommended &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; SNO?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Single Node OpenShift is particularly well-suited &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; several use cases and environments, including:                                                                                                             
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; 1. **Edge computing scenarios:** Due to its small footprint and ability to operate autonomously, SNO is ideal &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; deployments that require minimal management overhead or where connectivity may be intermittent  or unavailable. This makes it especially useful in telecommunication service provider &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;TSP&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; Radio Access Network &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;RAN&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; implementations, which often have stringent latency requirements.                         
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; 2. **Resource-intensive workloads:** SNO can handle demanding tasks that consume the majority of available system resources, ensuring optimal performance even when facing heavy loads. This is particularly  useful in scenarios where users need to deploy and manage their workloads with minimal external intervention or dependency on centralized management clusters.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Not bad!! ;) it learnt!!&lt;/p&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;InstructLab is settling the bases for a new community with tools that would be really awesome for create custom LLMs. Without needing to have tech/AI skills. Imagine modeling the knowledge of your company, or your personal documents and notes inside the taxonomy. You can build your own model and run it locally, preventing any possible issue about privacy.&lt;/p&gt;
&lt;p&gt;In this blog entry I have summarized my experience. But there are many tries and error behind. The tools is easy to use, it could looks like magic, but of course there is no magic. Your qna has to be well structured (and Markdown as much plain as possible), Linux with CPU is tricky, how may instructions your generated? how many epochs?etc. But, in general, the feeling and the experience is like a new world of opportunities.&lt;/p&gt;
&lt;p&gt;So, lets contribute to this awesome new community!! :)&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
